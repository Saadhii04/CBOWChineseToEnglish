{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAQDki6rwVxr",
        "outputId": "a2a78d1a-3fdc-4b2f-a1e1-7a47315b5efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "nltk.data.path.append('.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import jieba\n",
        "\n",
        "path = \"sample.txt\"\n",
        "\n",
        "with open(path, 'r', encoding='utf-8') as file:\n",
        "    data = file.read()\n",
        "\n",
        "data = re.sub(r'[，。！？；]', '。', data)\n",
        "\n",
        "data_tokens = jieba.cut(data)\n",
        "\n",
        "data_tokens = [token.lower() for token in data_tokens if token.isalpha() or token == '。']\n",
        "\n",
        "print(data_tokens[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mgCdT-xwgt7",
        "outputId": "f6522960-4149-495b-bdfb-0bca11419943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.530 seconds.\n",
            "DEBUG:jieba:Loading model cost 1.530 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['中国', '是', '世界', '上', '历史悠久', '且', '文化', '丰富', '的', '国家']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The Number of tokens = {len(data_tokens)} \\n {data[:20]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OO9WDZFwgr2",
        "outputId": "3147ec77-2d0e-468f-92b1-59b0043f5056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Number of tokens = 389 \n",
            " 中国是世界上历史悠久且文化丰富的国家之一\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist = nltk.FreqDist(word for word in data_tokens)\n",
        "print(\"Size of vocabulary: \",len(fdist) )\n",
        "print(\"Most frequent tokens: \",fdist.most_common(20) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5eV7Lmbwgpd",
        "outputId": "f9cb2fb2-4aef-426a-9112-64b0b45deb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary:  182\n",
            "Most frequent tokens:  [('。', 40), ('的', 34), ('和', 18), ('中国', 16), ('了', 11), ('在', 7), ('上', 5), ('历史', 5), ('其', 5), ('全球', 5), ('世界', 4), ('文化', 4), ('国家', 4), ('经济', 4), ('如', 4), ('发展', 4), ('等', 4), ('重要', 4), ('之一', 3), ('悠久', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "\n",
        "def get_dict(data_tokens):\n",
        "    words = sorted(list(set(data_tokens)))\n",
        "    word2Ind = {word: idx for idx, word in enumerate(words)}\n",
        "    Ind2word = {idx: word for idx, word in enumerate(words)}\n",
        "    return word2Ind, Ind2word"
      ],
      "metadata": {
        "id": "VXHqBBkDwgnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"sample.txt\"\n",
        "\n",
        "with open(path, 'r', encoding='utf-8') as file:\n",
        "    data = file.read()\n",
        "\n",
        "data_tokens = list(jieba.cut(data))\n",
        "\n",
        "word2Ind, Ind2word = get_dict(data_tokens)\n",
        "\n",
        "V = len(word2Ind)\n",
        "print(\"Size of vocabulary: \", V)\n",
        "\n",
        "print(\"Index of '在' in vocabulary: \", word2Ind.get('在'))\n",
        "print(\"Word corresponding to index 0: \", Ind2word.get(68))\n",
        "\n",
        "\n",
        "print(\"Index of the word '中国' :  \",word2Ind['中国'] )\n",
        "print(\"Word which has index 2743:  \",Ind2word[68] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnQ0cCwGwgkl",
        "outputId": "2df80190-fab7-48f8-f701-bf48f91f457b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary:  185\n",
            "Index of '在' in vocabulary:  68\n",
            "Word corresponding to index 0:  在\n",
            "Index of the word '中国' :   14\n",
            "Word which has index 2743:   在\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def init_parameters(N: int, V: int, random_seed: int = 1) -> dict:\n",
        "    np.random.seed(random_seed)\n",
        "    parameters = {}\n",
        "\n",
        "    # Assuming V represents the vocabulary size in Mandarin (characters or tokens)\n",
        "    parameters[\"W1\"] = np.random.rand(N, V)\n",
        "    parameters[\"b1\"] = np.zeros(shape=(N, 1))\n",
        "\n",
        "    # W2 would typically map from the hidden layer (N) back to the vocabulary size (V)\n",
        "    parameters[\"W2\"] = np.random.rand(V, N)\n",
        "    parameters[\"b2\"] = np.zeros(shape=(V, 1))\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "rGSLezndwgh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_N = 4\n",
        "tmp_V = 10\n",
        "params = init_parameters(tmp_N,tmp_V)\n",
        "assert params['W1'].shape == ((tmp_N,tmp_V))\n",
        "assert params['W2'].shape == ((tmp_V,tmp_N))\n",
        "print(f\"W1 shape: {params['W1'].shape}\")\n",
        "print(f\"W2 shape: {params['W2'].shape}\")\n",
        "print(f\"b1 shape: {params['b1'].shape}\")\n",
        "print(f\"b2 shape: {params['b2'].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl1S6DcWwgfX",
        "outputId": "8632cb56-efd2-48f9-e5e6-4b412d8e0490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 shape: (4, 10)\n",
            "W2 shape: (10, 4)\n",
            "b1 shape: (4, 1)\n",
            "b2 shape: (10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union\n",
        "\n",
        "def softmax(z: Union[float, List[float]]) -> Union[float, List[float]]:\n",
        "    y = np.exp(z) / np.sum(np.exp(z), 0, keepdims=True)\n",
        "\n",
        "    return y\n",
        "\n",
        "def sigmoid(z: Union[float, List[float]]) -> Union[float, List[float]]:\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def relu(z: Union[float, List[float]]) -> Union[float, List[float]]:\n",
        "    return np.maximum(z, 0)\n"
      ],
      "metadata": {
        "id": "pSBMW7L4wgc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = np.array([[1,2,3],\n",
        "                [1,1,1]\n",
        "               ])\n",
        "tmp_sm = softmax(tmp)\n",
        "display(tmp_sm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Mm3XsAklwgag",
        "outputId": "8cc8b67c-525f-47fd-a725-f5a3430ae5dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0.5       , 0.73105858, 0.88079708],\n",
              "       [0.5       , 0.26894142, 0.11920292]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(x: np.ndarray, params: dict) -> tuple:\n",
        "    z1 = np.dot(params['W1'], x) + params['b1']\n",
        "    h = relu(z1)\n",
        "\n",
        "    z2 = np.dot(params['W2'], h) + params['b2']\n",
        "    y_hat = z2\n",
        "\n",
        "    return y_hat, h"
      ],
      "metadata": {
        "id": "A0lVmU4Ex3Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_x = np.array([[0,1,0,0,0,0,0,0,0,0]]).T\n",
        "tmp_z, tmp_h = forward_propagation(tmp_x, params)\n",
        "print(\"call forward_prop\")\n",
        "print()\n",
        "# Look at output\n",
        "print(f\"z has shape {tmp_z.shape}\")\n",
        "print(\"z has values:\")\n",
        "print(tmp_z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWjR7qW4x3J9",
        "outputId": "8607a27f-8553-4de0-f76d-93fcc4adab6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call forward_prop\n",
            "\n",
            "z has shape (10, 1)\n",
            "z has values:\n",
            "[[1.82887324]\n",
            " [1.38466287]\n",
            " [0.60100484]\n",
            " [0.83284144]\n",
            " [1.37936774]\n",
            " [1.20420827]\n",
            " [1.26273995]\n",
            " [2.0149547 ]\n",
            " [1.10825153]\n",
            " [1.93910889]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y_hat: np.ndarray, y: np.ndarray,\n",
        "                       batch_size: int) -> Union[float, List[float]]:\n",
        "    logprobs = np.multiply(np.log(y_hat), y)\n",
        "    cost = -1/batch_size * np.sum(logprobs)\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost\n",
        "\n",
        "\n",
        "\n",
        "def backward_propagation(x: np.ndarray, y: np.ndarray, y_hat: np.ndarray, h: np.ndarray,\n",
        "                         params: dict, batch_size: int) -> dict:\n",
        "    grads_params = {}\n",
        "\n",
        "    l1 = np.dot(params['W2'].T ,(y_hat - y))\n",
        "\n",
        "    grads_params['W1'] = np.dot(l1, x.T) / batch_size\n",
        "    grads_params['W2'] = np.dot((y_hat - y), h.T) / batch_size\n",
        "\n",
        "    grads_params['b1'] = np.sum(l1, axis=1, keepdims= True) / batch_size\n",
        "    grads_params['b2'] = np.sum((y_hat - y), axis= 1, keepdims= True) / batch_size\n",
        "\n",
        "    return grads_params"
      ],
      "metadata": {
        "id": "rb6Og0V9x3H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import jieba\n",
        "\n",
        "def get_idx(words, word2Ind):\n",
        "    idx = []\n",
        "    for word in words:\n",
        "        idx.append(word2Ind[word])\n",
        "    return idx\n",
        "\n",
        "def pack_idx_with_frequency(context_words, word2Ind):\n",
        "    freq_dict = defaultdict(int)\n",
        "    for word in context_words:\n",
        "        freq_dict[word] += 1\n",
        "    idxs = get_idx(context_words, word2Ind)\n",
        "    packed = []\n",
        "    for i in range(len(idxs)):\n",
        "        idx = idxs[i]\n",
        "        freq = freq_dict[context_words[i]]\n",
        "        packed.append((idx, freq))\n",
        "    return packed\n",
        "\n",
        "def get_vectors(data, word2Ind, V, C):\n",
        "    i = C\n",
        "    while True:\n",
        "        y = np.zeros(V)\n",
        "        x = np.zeros(V)\n",
        "        center_word = data[i]\n",
        "        y[word2Ind[center_word]] = 1\n",
        "        context_words = data[(i - C) : i] + data[(i + 1) : (i + C + 1)]\n",
        "        num_ctx_words = len(context_words)\n",
        "        for idx, freq in pack_idx_with_frequency(context_words, word2Ind):\n",
        "            x[idx] = freq / num_ctx_words\n",
        "        yield x, y\n",
        "        i += 1\n",
        "        if i >= len(data) - C:\n",
        "            print(\"i is being set to\", C)\n",
        "            i = C\n",
        "\n",
        "def get_batches(data, word2Ind, V, C, batch_size):\n",
        "    batch_x = []\n",
        "    batch_y = []\n",
        "    for x, y in get_vectors(data, word2Ind, V, C):\n",
        "        if len(batch_x) < batch_size:\n",
        "            batch_x.append(x)\n",
        "            batch_y.append(y)\n",
        "        else:\n",
        "            yield np.array(batch_x).T, np.array(batch_y).T\n",
        "            batch_x = []\n",
        "            batch_y = []"
      ],
      "metadata": {
        "id": "qcQNCepNx3FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(data: list[str], word2Ind: dict, N: int,\n",
        "                     V: int, epochs: int, alpha: float = 0.03,\n",
        "                     random_seed: int = 282) -> dict:\n",
        "\n",
        "    param = init_parameters(N, V, random_seed=random_seed)\n",
        "\n",
        "    batch_size = 128\n",
        "    epoch = 0\n",
        "    C = 2\n",
        "\n",
        "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
        "\n",
        "        z, h = forward_propagation(x, param)\n",
        "        yhat = softmax(z)\n",
        "\n",
        "        cost = cross_entropy_loss(yhat, y, batch_size)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Iteration: {epoch + 1} Cost: {cost:.6f}\")\n",
        "\n",
        "        grads = backward_propagation(x, y, yhat, h, param, batch_size)\n",
        "\n",
        "        param['W1'] -= alpha * grads['W1']\n",
        "        param['W2'] -= alpha * grads['W2']\n",
        "        param['b1'] -= alpha * grads['b1']\n",
        "        param['b2'] -= alpha * grads['b2']\n",
        "\n",
        "        epoch += 1\n",
        "        if epoch == epochs:\n",
        "            break\n",
        "        if epoch % 100 == 0:\n",
        "            alpha *= 0.66\n",
        "\n",
        "    return param\n",
        "\n",
        "\n",
        "\n",
        "C = 2\n",
        "N = 50\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "num_iters = 150\n",
        "print(\"Call gradient_descent\")\n",
        "params = gradient_descent(data, word2Ind, N, V, num_iters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHKe9Ebyx3Dd",
        "outputId": "1ad73221-64b8-4046-fe5e-102fb8d77362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Call gradient_descent\n",
            "i is being set to 2\n",
            "Iteration: 10 Cost: 5.977854\n",
            "i is being set to 2\n",
            "i is being set to 2\n",
            "Iteration: 20 Cost: 6.006075\n",
            "i is being set to 2\n",
            "i is being set to 2\n",
            "Iteration: 30 Cost: 6.037121\n",
            "i is being set to 2\n",
            "i is being set to 2\n",
            "Iteration: 40 Cost: 5.966813\n",
            "i is being set to 2\n",
            "i is being set to 2\n",
            "Iteration: 50 Cost: 5.864473\n",
            "i is being set to 2\n",
            "i is being set to 2\n",
            "Iteration: 60 Cost: 5.844921\n",
            "i is being set to 2\n",
            "i is being set to 2\n",
            "Iteration: 70 Cost: 5.797549\n",
            "i is being set to 2\n",
            "Iteration: 80 Cost: 5.738867\n",
            "i is being set to 2\n",
            "i is being set to 2\n",
            "Iteration: 90 Cost: 5.619266\n",
            "i is being set to 2\n",
            "i is being set to 2\n",
            "Iteration: 100 Cost: 5.624046\n",
            "i is being set to 2\n",
            "i is being set to 2\n",
            "Iteration: 110 Cost: 5.600146\n",
            "i is being set to 2\n",
            "i is being set to 2\n",
            "Iteration: 120 Cost: 5.590284\n",
            "i is being set to 2\n",
            "i is being set to 2\n",
            "Iteration: 130 Cost: 5.626671\n",
            "i is being set to 2\n",
            "i is being set to 2\n",
            "Iteration: 140 Cost: 5.582384\n",
            "i is being set to 2\n",
            "i is being set to 2\n",
            "Iteration: 150 Cost: 5.584737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-47vRu8qx3A_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}